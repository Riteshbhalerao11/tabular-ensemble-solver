{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12465803,"sourceType":"datasetVersion","datasetId":7860006}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\"Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays.*\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:16.700562Z","iopub.execute_input":"2025-07-13T18:55:16.701062Z","iopub.status.idle":"2025-07-13T18:55:16.707862Z","shell.execute_reply.started":"2025-07-13T18:55:16.701039Z","shell.execute_reply":"2025-07-13T18:55:16.707357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q optuna tabm rtdl_num_embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:16.709118Z","iopub.execute_input":"2025-07-13T18:55:16.709359Z","iopub.status.idle":"2025-07-13T18:55:16.720725Z","shell.execute_reply.started":"2025-07-13T18:55:16.709336Z","shell.execute_reply":"2025-07-13T18:55:16.719915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed=42):\n    import os\n    import random\n    import numpy as np\n    import torch\n\n    # Python built-ins\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # NumPy\n    np.random.seed(seed)\n\n    # PyTorch CPU and GPU\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n\n    # CuDNN (may slow down training, but ensures determinism)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Extra safety for CUDA (PyTorch 1.8+)\n    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # \":4096:8\" also valid\n\n    # Reproducibility for newer versions of PyTorch\n    try:\n        torch.use_deterministic_algorithms(True)\n    except Exception:\n        pass  # older PyTorch versions may not support this\n\n    print(f\"✅ All libraries seeded with seed = {seed}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_everything()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport time\nimport math\nimport random\nfrom tqdm import trange\nfrom copy import deepcopy\nfrom typing import NamedTuple\n\nimport optuna\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim\nfrom torch import Tensor\n\n# import rtdl_num_embeddings\nfrom  rtdl_num_embeddings import compute_bins, PiecewiseLinearEmbeddings\nimport scipy.special\nimport tabm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:16.721847Z","iopub.execute_input":"2025-07-13T18:55:16.722088Z","iopub.status.idle":"2025-07-13T18:55:19.480663Z","shell.execute_reply.started":"2025-07-13T18:55:16.722066Z","shell.execute_reply":"2025-07-13T18:55:19.480058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_volume_weighted_component_features(X):\n    \"\"\"\n    Computes individual volume-weighted features WjPk = Componentj_fraction * Componentj_Propertyk\n    for j in 1..5 and k in 1..10 (total 50 features).\n    \"\"\"\n    features = {}\n    for comp_idx in range(1, 6):  # Components 1–5\n        for prop_idx in range(1, 11):  # Properties 1–10\n            vol_col = f'Component{comp_idx}_fraction'\n            prop_col = f'Component{comp_idx}_Property{prop_idx}'\n            feat_name = f'W{comp_idx}P{prop_idx}'\n            features[feat_name] = X[vol_col] * X[prop_col]\n    return pd.DataFrame(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.481384Z","iopub.execute_input":"2025-07-13T18:55:19.481789Z","iopub.status.idle":"2025-07-13T18:55:19.486846Z","shell.execute_reply.started":"2025-07-13T18:55:19.481763Z","shell.execute_reply":"2025-07-13T18:55:19.486090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_dir = \"/kaggle/working/TabM_models\"\nTARGETS = [f\"BlendProperty{i}\" for i in range(1, 11)]\nBASE_PATH = r\"/kaggle/input/shell25/data (5)/updated\"\nfi_path = r\"/kaggle/input/shell25/data (4)/feature_importance\"\nN_TRIALS = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.488743Z","iopub.execute_input":"2025-07-13T18:55:19.488917Z","iopub.status.idle":"2025-07-13T18:55:19.507610Z","shell.execute_reply.started":"2025-07-13T18:55:19.488901Z","shell.execute_reply":"2025-07-13T18:55:19.506900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_data(target, selective=False, skew_thresh=0.5, kurt_thresh=3.5):\n    # Load train and val sets\n    X_train = pd.read_csv(f\"{BASE_PATH}/train/{target}_X.csv\")\n    y_train = pd.read_csv(f\"{BASE_PATH}/train/{target}_y.csv\")\n    X_val = pd.read_csv(f\"{BASE_PATH}/val/{target}_X.csv\")\n    y_val = pd.read_csv(f\"{BASE_PATH}/val/{target}_y.csv\")\n\n    # Feature engineering\n    X_train = pd.concat([X_train, compute_volume_weighted_component_features(X_train)], axis=1)\n    X_val = pd.concat([X_val, compute_volume_weighted_component_features(X_val)], axis=1)\n\n    # Feature selection\n    if selective:\n        df = pd.read_csv(os.path.join(fi_path, f\"{target}.csv\"))\n        cols = df[df[\"importance\"] > 0.01].iloc[:, 0].tolist()\n        # print(cols)\n        X_train = X_train[cols]\n        X_val = X_val[cols]\n\n    # Separate out fraction-based columns\n    fraction_cols = [col for col in X_train.columns if \"fraction\" in col.lower()]\n    non_fraction_cols = [col for col in X_train.columns if col not in fraction_cols]\n\n    # Analyze distribution statistics\n    feature_stats = pd.DataFrame({\n        'skewness': X_train[non_fraction_cols].apply(skew, nan_policy='omit'),\n        'kurtosis': X_train[non_fraction_cols].apply(kurtosis, nan_policy='omit'),\n        'std': X_train[non_fraction_cols].std(),\n    })\n\n\n    # Initialize scaled DataFrames\n    X_train_scaled = X_train.copy()\n    X_val_scaled = X_val.copy()\n\n    for col in non_fraction_cols:\n        col_vals = X_train[[col]].values\n\n        if feature_stats.loc[col, 'std'] == 0.0:\n            col_vals += np.random.normal(0.0, 1e-5, size=col_vals.shape)\n\n        use_quantile = (\n            abs(feature_stats.loc[col, 'skewness']) > skew_thresh or\n            feature_stats.loc[col, 'kurtosis'] > kurt_thresh\n        )\n\n        if use_quantile:\n            scaler = QuantileTransformer(\n                n_quantiles=max(min(len(col_vals) // 30, 1000), 20),\n                output_distribution='normal',\n                subsample=10**9\n            )\n        else:\n            scaler = StandardScaler()\n\n        X_train_scaled[col] = scaler.fit_transform(col_vals).ravel()\n        X_val_scaled[col] = scaler.transform(X_val[[col]].values).ravel()\n\n    # y transformation based on skew\n    y_vals = y_train.values.ravel()\n    y_skew = skew(y_vals)\n    should_transform_y = abs(y_skew) > skew_thresh\n\n    if should_transform_y:\n        y_transformer = PowerTransformer(method=\"yeo-johnson\")\n        y_train_transformed = y_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n        y_val_transformed = y_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n        # print(f\"⚠️ Applied PowerTransformer to target '{target}' (skewness: {y_skew:.2f})\")\n    else:\n        y_transformer = StandardScaler()\n        y_train_transformed = y_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n        y_val_transformed = y_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n        # print(f\"✅ Applied StandardScaler to target '{target}' (skewness: {y_skew:.2f})\")\n\n    return X_train_scaled, y_train_transformed, X_val_scaled, y_val_transformed, y_transformer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:06:12.091139Z","iopub.status.idle":"2025-07-13T19:06:12.091523Z","shell.execute_reply.started":"2025-07-13T19:06:12.091348Z","shell.execute_reply":"2025-07-13T19:06:12.091365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')\n\n\namp_dtype = (\n    torch.bfloat16\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n    else torch.float16\n    if torch.cuda.is_available()\n    else None\n)\n\n\namp_enabled = amp_dtype is not None\ngrad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n\n# torch.compile\ncompile_model = False\n\n# fmt: off\nprint(f'Device:        {device.type.upper()}')\nprint(f'AMP:           {amp_enabled}{f\" ({amp_dtype})\"if amp_enabled else \"\"}')\nprint(f'torch.compile: {compile_model}')\n# fmt: on","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.520150Z","iopub.execute_input":"2025-07-13T18:55:19.520380Z","iopub.status.idle":"2025-07-13T18:55:19.679115Z","shell.execute_reply.started":"2025-07-13T18:55:19.520364Z","shell.execute_reply":"2025-07-13T18:55:19.678439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model(model_params, X_train):\n\n    if model_params['tree_binning']:\n        bins = compute_bins(\n            X_train.cpu(),\n            n_bins=model_params['bins_emb'],\n            y=Y_train,\n            regression=True,\n            tree_kwargs=model_params.get('tree_kwargs', {'min_samples_leaf': 64}),\n            verbose=False\n        )\n    else:\n        bins = compute_bins(X_train, n_bins=model_params['bins_emb'])\n\n    num_embeddings = PiecewiseLinearEmbeddings(\n        bins=bins,\n        d_embedding=model_params['d_emb'],\n        activation=model_params['act_emb'],\n        version='B'\n    )\n\n    model = tabm.TabM.make(\n        n_num_features=X_train.shape[1],\n        d_out=model_params[\"d_out\"],\n        num_embeddings=num_embeddings,\n        n_blocks=model_params[\"n_blocks\"],\n        d_block=model_params[\"d_block\"],\n        dropout=model_params[\"dropout\"],\n        activation=model_params[\"activation\"],\n        k=32,\n        arch_type=model_params[\"arch_type\"],\n        start_scaling_init=model_params[\"start_scaling_init\"],\n    ).to(device)\n    \n    # model = torch.compile(model)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=model_params['lr'], weight_decay=model_params['decay'])\n\n    return model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.679874Z","iopub.execute_input":"2025-07-13T18:55:19.680150Z","iopub.status.idle":"2025-07-13T18:55:19.686094Z","shell.execute_reply.started":"2025-07-13T18:55:19.680130Z","shell.execute_reply":"2025-07-13T18:55:19.685459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\ndef apply_model(model, data: Tensor, idx: Tensor) -> Tensor:\n    return (\n        model(\n            data[idx],\n            None,\n        )\n        .squeeze(-1)  # Remove the last dimension for regression tasks.\n        .float()\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.686879Z","iopub.execute_input":"2025-07-13T18:55:19.687132Z","iopub.status.idle":"2025-07-13T18:55:19.702904Z","shell.execute_reply.started":"2025-07-13T18:55:19.687110Z","shell.execute_reply":"2025-07-13T18:55:19.702280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"share_training_batches = True\n\ndef loss_fn(y_pred: Tensor, y_true: Tensor, model, delta: float = 1.0) -> Tensor:\n    y_pred = y_pred.flatten(0, 1)\n\n    if share_training_batches:\n        y_true = y_true.repeat_interleave(model.backbone.k)\n    else:\n        y_true = y_true.flatten(0, 1)\n\n    return F.huber_loss(y_pred, y_true, delta=delta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.703546Z","iopub.execute_input":"2025-07-13T18:55:19.703741Z","iopub.status.idle":"2025-07-13T18:55:19.721537Z","shell.execute_reply.started":"2025-07-13T18:55:19.703727Z","shell.execute_reply":"2025-07-13T18:55:19.720804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.inference_mode()\ndef evaluate(x_data: Tensor, y_data: Tensor, model, y_scaler=None) -> float:\n    model.eval()\n    eval_batch_size = 64\n    y_pred: np.ndarray = (\n        torch.cat(\n            [\n                apply_model(model, x_data, idx)\n                for idx in torch.arange(len(x_data), device=device).split(eval_batch_size)\n            ]\n        )\n        .cpu()\n        .numpy()\n    )\n    y_pred = y_pred.mean(1)\n\n    if y_scaler is not None:\n        y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n\n    y_true = y_data.cpu().numpy()\n\n    score = mean_absolute_percentage_error(y_true, y_pred)\n    return float(score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.723678Z","iopub.execute_input":"2025-07-13T18:55:19.723968Z","iopub.status.idle":"2025-07-13T18:55:19.736229Z","shell.execute_reply.started":"2025-07-13T18:55:19.723951Z","shell.execute_reply":"2025-07-13T18:55:19.735618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    X_train, Y_train, \n    X_val, Y_val, \n    Y_scaler, \n    model, optimizer, \n    batch_size, delta, \n    patience=50, \n    verbose=False\n):\n    device = X_train.device\n    train_size = X_train.shape[0]\n    n_epochs = 2000\n\n    best_val = float('inf')  # because we're minimizing\n    best_state = deepcopy(model.state_dict())\n    remaining_patience = patience\n\n    progress_bar = trange(n_epochs, disable=not verbose)\n    for epoch in progress_bar:\n        batches = (\n            torch.randperm(train_size, device=device).split(batch_size)\n            if share_training_batches\n            else torch.rand((train_size, model.backbone.k), device=device)\n                 .argsort(dim=0).split(batch_size, dim=0)\n        )\n\n        for batch_idx in batches:\n            model.train()\n            optimizer.zero_grad()\n            loss = loss_fn(apply_model(model, X_train, batch_idx), Y_train[batch_idx], model, delta)\n\n            if grad_scaler is None:\n                loss.backward()\n                optimizer.step()\n            else:\n                grad_scaler.scale(loss).backward()\n                grad_scaler.step(optimizer)\n                grad_scaler.update()\n\n        val_metric = evaluate(X_val, Y_val, model, Y_scaler)\n\n        if verbose:\n            progress_bar.set_description(f\"Epoch {epoch+1}\")\n            progress_bar.set_postfix(val_loss=val_metric)\n\n        if val_metric < best_val:\n            best_val = val_metric\n            best_state = deepcopy(model.state_dict())\n            remaining_patience = patience\n        else:\n            remaining_patience -= 1\n\n        if remaining_patience < 0:\n            break\n\n    model.load_state_dict(best_state)\n    return best_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.736928Z","iopub.execute_input":"2025-07-13T18:55:19.737611Z","iopub.status.idle":"2025-07-13T18:55:19.749481Z","shell.execute_reply.started":"2025-07-13T18:55:19.737583Z","shell.execute_reply":"2025-07-13T18:55:19.748730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial, target):\n    model_params = {\n        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n        'selective': trial.suggest_categorical(\"selective\", [True, False]),\n        \"decay\": trial.suggest_float(\"decay\", 1e-6, 0.5, log=True),\n        \"bins_emb\": trial.suggest_int(\"bins_emb\", 16, 128),\n        \"d_emb\": trial.suggest_categorical(\"d_emb\", [8, 16, 32, 64]),\n        \"batch_size\": trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n        \"act_emb\": trial.suggest_categorical(\"act_emb\", [True, False]),\n        \"n_blocks\": trial.suggest_int(\"n_blocks\", 2, 6),\n        \"d_block\": trial.suggest_categorical(\"d_block\", [256, 512, 1024, 2048]),\n        \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n        \"activation\": trial.suggest_categorical(\"activation\", [\"ReLU\", \"GELU\", \"SiLU\", \"ELU\"]),\n        \"arch_type\": \"tabm\",\n        \"start_scaling_init\": trial.suggest_categorical(\"start_scaling_init\", [\"normal\", \"random-signs\"]),\n        \"d_out\": 1,\n        \"tree_binning\": False,\n        \"huber_delta\": trial.suggest_float(\"huber_delta\", 0.1, 3.0, log=True)\n    }\n\n    X_train, Y_train, X_val, Y_val, Y_scaler = get_data(target, selective=model_params['selective'])\n    X_train = torch.tensor(X_train.values, dtype=torch.float32, device=device)\n    Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device)\n    X_val   = torch.tensor(X_val.values,   dtype=torch.float32, device=device)\n    Y_val   = torch.tensor(Y_val, dtype=torch.float32, device=device)\n\n    model, optimizer = get_model(model_params, X_train)\n    best_val = train(\n        X_train, Y_train,\n        X_val, Y_val, Y_scaler,\n        model, optimizer,\n        batch_size=model_params['batch_size'],\n        delta=model_params['huber_delta'],\n        patience=50\n    )\n\n\n    return best_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.750284Z","iopub.execute_input":"2025-07-13T18:55:19.750524Z","iopub.status.idle":"2025-07-13T18:55:19.766578Z","shell.execute_reply.started":"2025-07-13T18:55:19.750508Z","shell.execute_reply":"2025-07-13T18:55:19.765815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:19.767402Z","iopub.execute_input":"2025-07-13T18:55:19.767634Z","iopub.status.idle":"2025-07-13T18:55:19.783358Z","shell.execute_reply.started":"2025-07-13T18:55:19.767611Z","shell.execute_reply":"2025-07-13T18:55:19.782785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for target in TARGETS[4:6]:\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(lambda trial: objective(trial, target), n_trials=N_TRIALS, n_jobs=1, show_progress_bar=True)\n\n    complete_params = {**study.best_params}\n    \n    print(f\"\\nBest MAPE for {target}: {study.best_value:.4f}\")\n    print(f\"Best params for {target}:\\n{study.best_params}\\n\")\n    \n    os.makedirs(model_dir, exist_ok=True)\n    with open(os.path.join(model_dir, f\"best_params_{target}_updated.json\"), \"w\") as f:\n        json.dump(complete_params, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:06:12.682560Z","iopub.execute_input":"2025-07-13T19:06:12.682842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}