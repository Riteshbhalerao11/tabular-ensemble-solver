{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751f3fe8-d106-49fd-8e66-7195cb2cb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Computing tree-based bins involves the conversion of the input PyTorch tensors to NumPy arrays.*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fceb571-d9f5-425d-8fa3-04704cad0e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from typing import NamedTuple\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "\n",
    "# import rtdl_num_embeddings\n",
    "from  rtdl_num_embeddings import compute_bins, PiecewiseLinearEmbeddings\n",
    "import scipy.special\n",
    "import tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadf5395-13fc-4c01-8a37-b09f88ecca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_volume_weighted_component_features(X):\n",
    "    \"\"\"\n",
    "    Computes individual volume-weighted features WjPk = Componentj_fraction * Componentj_Propertyk\n",
    "    for j in 1..5 and k in 1..10 (total 50 features).\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    for comp_idx in range(1, 6):  # Components 1–5\n",
    "        for prop_idx in range(1, 11):  # Properties 1–10\n",
    "            vol_col = f'Component{comp_idx}_fraction'\n",
    "            prop_col = f'Component{comp_idx}_Property{prop_idx}'\n",
    "            feat_name = f'W{comp_idx}P{prop_idx}'\n",
    "            features[feat_name] = X[vol_col] * X[prop_col]\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce243a9-a1e4-4714-9e97-fb3ab97d92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/pscratch/sd/r/ritesh11/temp_dir/TabM_models\"\n",
    "TARGETS = [f\"BlendProperty{i}\" for i in range(1, 11)]\n",
    "BASE_PATH = \"/pscratch/sd/r/ritesh11/temp_dir/dataset\"\n",
    "fi_path = \"/pscratch/sd/r/ritesh11/temp_dir/feature_importance\"\n",
    "N_TRIALS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136e79f2-90fd-4736-a0da-ccf655a26136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(target, selective=False, skew_thresh=0.5, kurt_thresh=3.5):\n",
    "    # Load train and val sets\n",
    "    X_train = pd.read_csv(f\"{BASE_PATH}/train/{target}_X.csv\")\n",
    "    y_train = pd.read_csv(f\"{BASE_PATH}/train/{target}_y.csv\")\n",
    "    X_val = pd.read_csv(f\"{BASE_PATH}/val/{target}_X.csv\")\n",
    "    y_val = pd.read_csv(f\"{BASE_PATH}/val/{target}_y.csv\")\n",
    "\n",
    "    # Feature engineering\n",
    "    X_train = pd.concat([X_train, compute_volume_weighted_component_features(X_train)], axis=1)\n",
    "    X_val = pd.concat([X_val, compute_volume_weighted_component_features(X_val)], axis=1)\n",
    "\n",
    "    # Feature selection\n",
    "    if selective:\n",
    "        df = pd.read_csv(os.path.join(fi_path, f\"{target}.csv\"))\n",
    "        cols = df[df[\"importance\"] > 0.01].iloc[:, 0].tolist()\n",
    "        # print(cols)\n",
    "        X_train = X_train[cols]\n",
    "        X_val = X_val[cols]\n",
    "\n",
    "    # Separate out fraction-based columns\n",
    "    fraction_cols = [col for col in X_train.columns if \"fraction\" in col.lower()]\n",
    "    non_fraction_cols = [col for col in X_train.columns if col not in fraction_cols]\n",
    "\n",
    "    # Analyze distribution statistics\n",
    "    feature_stats = pd.DataFrame({\n",
    "        'skewness': X_train[non_fraction_cols].apply(skew, nan_policy='omit'),\n",
    "        'kurtosis': X_train[non_fraction_cols].apply(kurtosis, nan_policy='omit'),\n",
    "        'std': X_train[non_fraction_cols].std(),\n",
    "    })\n",
    "\n",
    "\n",
    "    # Initialize scaled DataFrames\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "\n",
    "    for col in non_fraction_cols:\n",
    "        col_vals = X_train[[col]].values\n",
    "\n",
    "        if feature_stats.loc[col, 'std'] == 0.0:\n",
    "            col_vals += np.random.normal(0.0, 1e-5, size=col_vals.shape)\n",
    "\n",
    "        use_quantile = (\n",
    "            abs(feature_stats.loc[col, 'skewness']) > skew_thresh or\n",
    "            feature_stats.loc[col, 'kurtosis'] > kurt_thresh\n",
    "        )\n",
    "\n",
    "        if use_quantile:\n",
    "            scaler = QuantileTransformer(\n",
    "                n_quantiles=max(min(len(col_vals) // 30, 1000), 20),\n",
    "                output_distribution='normal',\n",
    "                subsample=10**9\n",
    "            )\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "        X_train_scaled[col] = scaler.fit_transform(col_vals).ravel()\n",
    "        X_val_scaled[col] = scaler.transform(X_val[[col]].values).ravel()\n",
    "\n",
    "    # y transformation based on skew\n",
    "    y_vals = y_train.values.ravel()\n",
    "    y_skew = skew(y_vals)\n",
    "    should_transform_y = abs(y_skew) > skew_thresh\n",
    "\n",
    "    if should_transform_y:\n",
    "        y_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "        y_train_transformed = y_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "        y_val_transformed = y_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "        # print(f\"⚠️ Applied PowerTransformer to target '{target}' (skewness: {y_skew:.2f})\")\n",
    "    else:\n",
    "        y_transformer = StandardScaler()\n",
    "        y_train_transformed = y_transformer.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "        y_val_transformed = y_transformer.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "        # print(f\"✅ Applied StandardScaler to target '{target}' (skewness: {y_skew:.2f})\")\n",
    "\n",
    "    return X_train_scaled, y_train_transformed, X_val_scaled, y_val_transformed, y_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675d34bc-8f37-4a63-a5b9-b53afe5fb44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           True (torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "\n",
    "\n",
    "amp_enabled = amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(f'Device:        {device.type.upper()}')\n",
    "print(f'AMP:           {amp_enabled}{f\" ({amp_dtype})\"if amp_enabled else \"\"}')\n",
    "print(f'torch.compile: {compile_model}')\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5f2a738-f127-40ff-9e35-81fa27e85d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_params, X_train):\n",
    "\n",
    "    if model_params['tree_binning']:\n",
    "        bins = compute_bins(\n",
    "            X_train.cpu(),\n",
    "            n_bins=model_params['bins_emb'],\n",
    "            y=Y_train,\n",
    "            regression=True,\n",
    "            tree_kwargs=model_params.get('tree_kwargs', {'min_samples_leaf': 64}),\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        bins = compute_bins(X_train, n_bins=model_params['bins_emb'])\n",
    "\n",
    "    num_embeddings = PiecewiseLinearEmbeddings(\n",
    "        bins=bins,\n",
    "        d_embedding=model_params['d_emb'],\n",
    "        activation=model_params['act_emb'],\n",
    "        version='B'\n",
    "    )\n",
    "\n",
    "    model = tabm.TabM.make(\n",
    "        n_num_features=X_train.shape[1],\n",
    "        d_out=model_params[\"d_out\"],\n",
    "        num_embeddings=num_embeddings,\n",
    "        n_blocks=model_params[\"n_blocks\"],\n",
    "        d_block=model_params[\"d_block\"],\n",
    "        dropout=model_params[\"dropout\"],\n",
    "        activation=model_params[\"activation\"],\n",
    "        k=32,\n",
    "        arch_type=model_params[\"arch_type\"],\n",
    "        start_scaling_init=model_params[\"start_scaling_init\"],\n",
    "    ).to(device)\n",
    "    \n",
    "    # model = torch.compile(model)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=model_params['lr'], weight_decay=model_params['decay'])\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf85f912-7192-4cfb-8a34-d475f7a0dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(model, data: Tensor, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[idx],\n",
    "            None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21cc5c3f-292e-4cb2-8dd5-2941cc8ef7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "share_training_batches = True\n",
    "\n",
    "def loss_fn(y_pred: Tensor, y_true: Tensor, model, delta: float = 1.0) -> Tensor:\n",
    "    y_pred = y_pred.flatten(0, 1)\n",
    "\n",
    "    if share_training_batches:\n",
    "        y_true = y_true.repeat_interleave(model.backbone.k)\n",
    "    else:\n",
    "        y_true = y_true.flatten(0, 1)\n",
    "\n",
    "    return F.huber_loss(y_pred, y_true, delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1595db23-b469-462d-bfd6-15b916becf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(x_data: Tensor, y_data: Tensor, model, y_scaler=None) -> float:\n",
    "    model.eval()\n",
    "    eval_batch_size = 64\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(model, x_data, idx)\n",
    "                for idx in torch.arange(len(x_data), device=device).split(eval_batch_size)\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    y_pred = y_pred.mean(1)\n",
    "\n",
    "    if y_scaler is not None:\n",
    "        y_pred = y_scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "\n",
    "    y_true = y_data.cpu().numpy()\n",
    "\n",
    "    score = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d622cb-14e3-44ff-9732-0277646edba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_val, Y_val, Y_scaler, model, optimizer, batch_size, delta, patience=100):\n",
    "    device = X_train.device\n",
    "    train_size = X_train.shape[0]\n",
    "    n_epochs = 2000\n",
    "\n",
    "    best_val = float('inf')  # <-- because we're minimizing\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    remaining_patience = patience\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batches = (\n",
    "            torch.randperm(train_size, device=device).split(batch_size)\n",
    "            if share_training_batches\n",
    "            else torch.rand((train_size, model.backbone.k), device=device)\n",
    "                 .argsort(dim=0).split(batch_size, dim=0)\n",
    "        )\n",
    "\n",
    "        for batch_idx in batches:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(apply_model(model,X_train, batch_idx), Y_train[batch_idx], model, delta)\n",
    "\n",
    "            if grad_scaler is None:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "        val_metric = evaluate(X_val, Y_val, model,  Y_scaler)  # lower is better now\n",
    "\n",
    "        if val_metric < best_val:\n",
    "            best_val = val_metric\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            remaining_patience = patience\n",
    "        else:\n",
    "            remaining_patience -= 1\n",
    "\n",
    "        if remaining_patience < 0:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc3632b-1c84-4d44-8d64-1fd6f0574fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, target):\n",
    "    model_params = {\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        'selective': trial.suggest_categorical(\"selective\", [True, False]),\n",
    "        \"decay\": trial.suggest_float(\"decay\", 1e-6, 0.5, log=True),\n",
    "        \"bins_emb\": trial.suggest_int(\"bins_emb\", 16, 128),\n",
    "        \"d_emb\": trial.suggest_categorical(\"d_emb\", [8, 16, 32, 64]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [8, 16, 32]),\n",
    "        \"act_emb\": trial.suggest_categorical(\"act_emb\", [True, False]),\n",
    "        \"n_blocks\": trial.suggest_int(\"n_blocks\", 2, 6),\n",
    "        \"d_block\": trial.suggest_categorical(\"d_block\", [256, 512, 1024, 2048]),\n",
    "        \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
    "        \"activation\" : trial.suggest_categorical(\"activation\", [\"ReLU\", \"GELU\", \"SiLU\",\"ELU\"]),\n",
    "        \"arch_type\": \"tabm\",\n",
    "        \"start_scaling_init\": trial.suggest_categorical(\"start_scaling_init\", [\"normal\",\"random-signs\"]),\n",
    "        \"d_out\": 1,\n",
    "        \"tree_binning\": False,\n",
    "        \"huber_delta\" : trial.suggest_float(\"huber_delta\", 0.1, 3.0, log=True)\n",
    "    }\n",
    "\n",
    "    X_train, Y_train, X_val, Y_val, Y_scaler = get_data(target,selective=model_params['selective'])\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32, device=device)\n",
    "    Y_train = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "    X_val   = torch.tensor(X_val.values,   dtype=torch.float32, device=device)\n",
    "    Y_val   = torch.tensor(Y_val,   dtype=torch.float32, device=device)\n",
    "\n",
    "    model, optimizer = get_model(model_params, X_train)\n",
    "    best_val = train(\n",
    "        X_train, Y_train,\n",
    "        X_val, Y_val,Y_scaler,\n",
    "        model, optimizer,\n",
    "        batch_size=model_params['batch_size'],\n",
    "        delta=model_params['huber_delta'],\n",
    "        patience=100\n",
    "    )\n",
    "\n",
    "    return best_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc4fe2f3-566f-4590-9fe1-80946fcc517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daec37a-aaba-4c5c-8fba-fc1483761cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c78e59f944451aacb747e00f8a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in TARGETS:\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, target), n_trials=N_TRIALS,\n",
    "                   n_jobs=16,show_progress_bar=True)\n",
    "\n",
    "    print(f\"\\nBest MAPE for {target}: {study.best_value:.4f}\")\n",
    "    print(f\"Best params for {target}:\\n{study.best_params}\\n\")\n",
    "    \n",
    "    complete_params = {**study.best_params}\n",
    "    \n",
    "    # Save best params (skip model training for now)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    with open(os.path.join(model_dir, f\"best_params_{target}.json\"), \"w\") as f:\n",
    "        json.dump(complete_params, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf1d03-0a01-4987-aa43-fd2a473e7749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_3.12)\n",
   "language": "python",
   "name": "myenv_3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
